"""
Privacy evaluation via Membership Inference Attack (MIA).
"""
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score


def membership_inference_attack(real_train, synthetic, real_test):
    """
    Perform MIA following Hayes et al. (2019).

    Args:
        real_train: Real training samples used as "members".
        synthetic: Synthetic samples generated by the model under evaluation.
        real_test: Real test samples (currently unused).

    Returns:
        float: MIA AUC (0.5 = perfect privacy, 1.0 = total memorization).
    """
    n_samples = min(len(real_train), len(synthetic))
    if n_samples < 20: return 0.5 # Not enough data for a reliable attack
    X_member = np.vstack([real_train[:n_samples], synthetic[:n_samples]])
    y_member = np.hstack([np.ones(n_samples), np.zeros(n_samples)])
    clf = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)
    X_tr, X_te, y_tr, y_te = train_test_split(X_member, y_member, test_size=0.3, random_state=42, stratify=y_member)
    clf.fit(X_tr, y_tr)
    y_pred_proba = clf.predict_proba(X_te)[:, 1]
    return float(roc_auc_score(y_te, y_pred_proba))

"""
Privacy evaluation using Differential Privacy accounting.
"""

from scipy.stats import norm

def compute_rdp_epsilon(steps, noise_multiplier, delta=1e-5):
    """
    Compute (ε, δ)-DP guarantee using Rényi Differential Privacy.
    
    Uses the "moments accountant" from Abadi et al. (2016).
    
    Args:
        steps: Number of training iterations
        noise_multiplier: σ in Gaussian mechanism (noise_scale / sensitivity)
        delta: Privacy parameter (typically 1e-5)
        
    Returns:
        epsilon: Privacy budget ε
    """
    # Simplified RDP calculation (for demonstration)
    # For production, use: https://github.com/tensorflow/privacy
    
    from scipy.special import comb
    
    orders = np.arange(2, 64)  # Rényi orders
    rdp = np.zeros_like(orders, dtype=float)
    
    for i, order in enumerate(orders):
        # RDP for single step
        rdp[i] = order / (2 * noise_multiplier**2)
    
    # Composition over T steps
    rdp_total = rdp * steps
    
    # Convert RDP to (ε, δ)-DP
    epsilon = rdp_total + np.log(1/delta) / (orders - 1)
    
    return epsilon.min()


def evaluate_privacy(X_real, X_synthetic, config):
    """
    Comprehensive privacy evaluation.
    
    Returns:
        dict: {
            'dcr_mean': Geodesic distance to closest real,
            'mia_auc': Membership inference attack success,
            'dp_epsilon': Formal DP guarantee
        }
    """
    # 1. Geodesic DCR
    dcr_mean = compute_geodesic_dcr(X_synthetic, X_real)
    
    # 2. MIA (existing code)
    mia_auc = membership_inference_attack(X_real, X_synthetic)
    
    # 3. Formal DP (if applicable)
    if 'dp_noise_multiplier' in config:
        dp_epsilon = compute_rdp_epsilon(
            steps=config.get('tabddpm_epochs', 1000),
            noise_multiplier=config['dp_noise_multiplier'],
            delta=1e-5
        )
    else:
        dp_epsilon = None
    
    return {
        'dcr_mean': dcr_mean,
        'mia_auc': mia_auc,
        'dp_epsilon': dp_epsilon
    }
